% !TeX root = RJwrapper.tex
\title{\pkg{casebase}: An Alternative Framework For Survival Analysis
and Comparison of Event Rates}
\author{by Sahir Rai Bhatnagar*, Maxime Turgeon*, Jesse Islam, James A.
Hanley, and Olli Saarela}

\maketitle

\abstract{%
In epidemiological studies of time-to-event data, a quantity of interest
to the clinician is their patient's risk of an event. However, methods
relying on time matching or risk-set sampling (including Cox regression)
eliminate the baseline hazard from the estimating function. As a
consequence, the focus has been on reporting hazard ratios instead of
survival curves. Indeed, reporting patient risk requires a separate
estimation of the baseline hazard. Using case-base sampling, Hanley \&
Miettinen (2009) explained how parametric hazard functions can be
estimated in continuous-time using logistic regression. Their approach
naturally leads to estimates of the survival function that are
smooth-in-time.\\
In this paper, we present the \CRANpkg{casebase} R package, a
comprehensive and flexible toolkit for parametric survival analysis. We
describe how the case-base framework can be used in more complex
settings: non-linear functions of time and non-proportional hazards,
competing risks, and variable selection. Our package also includes an
extensive array of visualization tools to complement the analysis. We
illustrate all these features through three different case studies.\\
* SRB and MT contributed equally to this work.
}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

The semiparametric Cox model has become the default approach to survival
analysis even though Cox himself later suggested he would prefer to
model the hazard function directly. In a 1994 interview with Professor
Nancy Reid, Sir David Cox was asked how he would model a set of censored
survival data, to which he responded: ``I think I would normally want to
tackle problems parametrically \ldots{} and if you want to do things
like predict the outcome for a particular patient, it's much more
convenient to do that parametrically'' \citep{reid1994conversation}.
Indeed, the most relevant quantity in a clinical setting is often the 5-
or 10-year risk of experiencing a certain event given the patient's
particular circumstances. Unfortunately, the most reported metric from a
Cox model is the (potentially time-dependent) hazard ratio (HR), which
ignores the duration of follow-up and is subject to selection bias
\citep{hernan2010hazards}. The covariate-adjusted survival curve
overcomes these limitations, and it is arguably a more important summary
measure to report than the HR. While stepwise survival curves can be
computed with the Cox model, they require a second step to separately
estimate the baseline hazard \citep{breslow1972discussion}.

Several authors have since pursued fully parametric approaches that made
the fitting of smooth survival curves much more transparent and
intuitive through generalized linear models. The key feature of these
procedures is splitting the time axis into discrete intervals. Whitehead
\citeyearpar{whitehead1980fitting} showed the equivalence between a Cox
model and a Poisson regression with a parameter for each event time;
Carstensen \citeyearpar{copenhagen2012needs} provides a nice exposition
of this equivalence with a real data example and supporting R code for
computing standard errors. Arjas \& Haara
\citeyearpar{arjas1987logistic} and Efron
\citeyearpar{efron1988logistic} treated each patient-day as a Bernoulli
random variable with probability equal to the discrete hazard rate. A
potential issue with these approaches is that the number of time bins
need to be chosen by the data analyst. On the one hand, a fine grouping
of times may result in few (or none) events per interval, which then
leads to instability in the Newton-Raphson procedure for estimation
\citep[Section 4.8]{kalbfleisch2011statistical}. On the other hand, a
coarse grouping could potentially mask nonlinear trends in the hazard
function.

Rather than discretizing time, Hanley \& Miettinen
\citeyearpar{hanley2009fitting} selected a discrete set of person-time
coordinates (`person-moments') in continuous time from all observed
follow-up experience constituting the study base. By doing so, they
obtained a likelihood expression for the hazard function that is
equivalent to that of logistic regression. More specifically, all
person-moments when the event of interest occurred are selected as the
case series, complemented by a randomly sampled base series of
person-moments serving as controls. This approach allows flexible
modeling of the hazard function by including time as a covariate
(e.g.~using splines or general additive models). Furthermore,
time-dependent covariates can be modeled through interactions with time.
In short, Hanley \& Miettinen \citeyearpar{hanley2009fitting} use the
well-understood logistic regression for directly modeling the hazard
function, without requiring a discrete-time model.

In this article, we present the \CRANpkg{casebase} R package
\citep{casebase-package} which implements the Hanley \& Miettinen
\citeyearpar{hanley2009fitting} approach for fitting fully parametric
hazard models and covariate-adjusted survival curves using the familiar
interface of the \code{glm} function. Our implementation allows for
straightforward extensions to other models such as penalized regression
for variable selection and competing-risk analysis. In addition, we
provide functions for exploratory data analysis and visualizing the
estimated quantities such as the hazard function, survival curve, and
their standard errors. The ultimate goal of our package is to make
fitting flexible hazards accessible to more end users with the hope that
they will favor reporting absolute risks over hazard ratios.

In what follows, we first recall some theoretical details on case-base
sampling and its use for estimating parametric hazard functions. We then
give a short review of existing R packages that implement comparable
features as \pkg{casebase}. Next, we provide some details about the
implementation of case-base sampling in our package, and we give a brief
survey of its main functions. This is followed by three case studies
that illustrate the flexibility and capabilities of \pkg{casebase}. We
show how the same framework can be used for non-linear functions of time
and non-proportional hazards, competing risks, and variable selection
via penalized regression. Finally, we end the article with a discussion
of the results and of future directions.

\hypertarget{theory}{%
\section{Theoretical details}\label{theory}}

As discussed in Hanley \& Miettinen \citeyearpar{hanley2009fitting}, the
key idea behind case-base sampling is to consider the entire study base
as an infinite collection of \emph{person moments}. These person moments
are indexed by both an individual in the study and a time point, and
therefore each person moment has a covariate profile, an exposure
status, and an outcome status (i.e.~whether the event happened) attached
to it. By comparing person moments at which an event occurred with
person moments at which no event occurred, we can extract information
about hazards and survival.

Therefore, we start by sampling all person moments at which the event
occurred; this collection of person moments is what Hanley \& Miettinen
call the \emph{case series}. The incidence of the case series is
dictated by the hazard function of interest. Next, we sample a finite
number of person moments at which no event occurred; this second
collection of person moment is what Hanley \& Miettinen call the
\emph{base series}. The sampling mechanism for the base series is left
at the discretion of the user, but in practice we find that sampling
uniformly from the study base provides both simplicity and good
performance. This is the default sampling mechanism in the package.

\hypertarget{likelihood-and-estimating-function}{%
\subsection{Likelihood and estimating
function}\label{likelihood-and-estimating-function}}

To describe the theoretical foundations of case-base sampling, we use
the framework of counting processes. In what follows, we abuse notation
slightly and omit any mention of \(\sigma\)-algebras. Instead, following
Aalen \textit{et al} \citeyearpar{aalen2008survival}, we use the
placeholder ``past'' to denote the past history of the corresponding
process. The reader interested in more details can refer to Saarela \&
Arjas \citeyearpar{saarela2015non} and Saarela
\citeyearpar{saarela2016case}. First, let \(N_{i}(t) \in \{0, 1\}\) be
counting processes corresponding to the event of interest for individual
\(i=1, \ldots,n\). For simplicity, we will consider Type I censoring due
to the end of follow-up at time \(\tau\) (the general case of
non-informative censoring is treated in Saarela
\citeyearpar{saarela2016case}). We assume a continuous time model, which
implies that the counting process jumps are less than or equal to one.
We are interested in modeling the hazard functions \(\lambda_{i}(t)\) of
the processes \(N_i(t)\), and which satisfy
\[\lambda_{i}(t) dt = E[dN_{i}(t)\mid \mathrm{past}].\] The processes
\(N_i(t)\) count the person moments from the \emph{case series}.

To complement the case series, we sample person moments for the base
series. To do so, we model the base series sampling mechanism using
non-homogeneous Poisson processes \(R_i(t) \in \{0, 1, 2, \ldots\}\);
the person-moments where \(dR_i(t) = 1\) constitute the base series. We
note that the same individual can contribute multiple person-moments to
the base series. The process \(Q_{i}(t) = R_i(t) + N_{i}(t)\) then
counts both the case and base series person-moments contributed by
individual \(i\). As mentioned above, the processes \(R_i(t)\) are
typically defined by the user via its intensity function \(\rho_i(t)\).
The process \(Q_{i}(t)\) is characterized by
\(E[dQ_{i}(t)\mid\mathrm{past}] = \lambda_{i}(t)dt + \rho_i(t)dt\).

If the hazard function \(\lambda_{i}(t; \theta)\) is parametrized in
terms of \(\theta\), we can define an estimator \(\hat{\theta}\) by
maximization of the likelihood expression
\[L_0(\theta) = \prod_{i=1}^n \exp\left\{ -\int_0^{\min(t_i,\tau)} \lambda_i(t; \theta) dt \right\} \prod_{i=1}^{n} \prod_{t\in[0,\tau)} \lambda_{i}(t;\theta)^{dN_{i}(t)},\]
where \(\prod_{t\in[0,u)}\) represents a product integral from \(0\) to
\(u\), and where \(t_i\) is the event time for individual \(i\).
However, the integral over time makes the computation and maximization
of \(L_0(\theta)\) challenging.

Case-base sampling allows us to avoid this integral. By conditioning on
a sampled person-moment, we get individual likelihood contributions of
the form
\[P(dN_{i}(t) \mid dQ_{i}(t) = 1,\mathrm{past}) \stackrel{\theta}{\propto} \frac{\lambda_{i}(t; \theta)^{dN_{i}(t)}}{\rho_i(t) + \lambda_{i}(t;\theta)}.\]
Therefore, we can define an estimating function for \(\theta\) as
follows: \begin{equation}
L(\theta) = \prod_{i=1}^{n} \prod_{t\in[0,\tau)} \left(\frac{\lambda_{i}(t; \theta)^{dN_{i}(t)}}{\rho_i(t) + \lambda_{i}(t;\theta)}\right)^{dQ_i(t)}. \label{eq:lik-function}
\end{equation} When a logarithmic link function is used for modeling the
hazard function, the above expression is of a logistic regression form
with an offset term \(\log(1/\rho_i(t))\). Note that the sampling units
selected in the case-base sampling mechanism are person-moments, rather
than individuals, and the parameters to be estimated are hazards or
hazard ratios rather than odds or odds ratios. Generally, an individual
can contribute more than one person-moment, and thus the terms in the
product integral are not independent. Nonetheless, Saarela
\citeyearpar{saarela2016case} showed that the logarithm of this
estimating function has mean zero at the true value \(\theta=\theta_0\),
and that the resulting estimator \(\hat{\theta}\) is asymptotically
normally distributed.

In Hanley \& Miettinen \citeyearpar{hanley2009fitting}, the authors
suggest sampling the base series \emph{uniformly} from the study base.
In terms of Poisson processes, their sampling strategy corresponds
essentially to a time-homogeneous Poisson process with hazard equal to
\(\rho_i(t) = b/B\), where \(b\) is the number of sampled observations
in the base series, and \(B\) is the total population-time for the study
base (e.g.~the sum of all individual follow-up times). More complex
examples are also possible; see for example Saarela \& Arjas
\citeyearpar{saarela2015non}, where the intensity functions for the
sampling mechanism are proportional to the cardiovascular disease event
rate given by the Framingham score. Non-uniform sampling mechanisms can
increase the efficiency of the case-base estimators.

\hypertarget{common-parametric-models}{%
\subsection{Common parametric models}\label{common-parametric-models}}

Let \(g(t; X)\) be the linear predictor such that
\(\log(\lambda(t;X)) = g(t; X)\). Different functions of \(t\) lead to
different parametric hazard models. The simplest of these models is the
one-parameter exponential distribution which is obtained by taking the
hazard function to be constant over the range of \(t\):

\begin{equation}
\log(\lambda(t; X)) = \beta_0 + \beta_1 X. \label{eq:exp}
\end{equation} In this model, the instantaneous failure rate is
independent of
\(t\).\footnote{The conditional chance of failure in a time interval of specified length is the same regardless of how long the individual has been in the study. This is also known as the \dfn{memoryless property} \citep{kalbfleisch2011statistical}.}

The Gompertz hazard model is given by including a linear term for time:

\begin{equation}
\log(\lambda(t; X)) = \beta_0 + \beta_1 t + \beta_2 X. \label{eq:gomp}
\end{equation}

Use of \(\log(t)\) yields the Weibull hazard which allows for a power
dependence of the hazard on time \citep{kalbfleisch2011statistical}:

\begin{equation}
\log(\lambda(t; X)) = \beta_0 + \beta_1 \log(t) + \beta_2 X. \label{eq:weibull}
\end{equation}

\hypertarget{competing-risk-analysis}{%
\subsection{Competing-risk analysis}\label{competing-risk-analysis}}

Case-base sampling can also be used in the context of competing-risk
analysis. Assuming there are \(J\) competing events, we can show that
each person-moment's contribution to the likelihood is of the form

\[\frac{\lambda_j(t)^{dN_j(t)}}{\rho(t) + \sum_{j=1}^J\lambda_j(t)},\]
where \(N_j(t)\) is the counting process associated with the event of
type \(j\) and \(\lambda_j(t)\) is the corresponding cause-specific
hazard function. As may be expected, this functional form is similar to
the terms appearing in the likelihood function for multinomial
regression.\footnote{Specifically, it corresponds to the following parametrization: \begin{align*} \log\left(\frac{P(Y=j \mid X)}{P(Y = J \mid X)}\right) = X^T\beta_j, \qquad j = 1,\ldots, J-1.\end{align*}}

\hypertarget{variable-selection}{%
\subsection{Variable selection}\label{variable-selection}}

To perform variable selection on the regression parameters
\(\theta \in \mathbb{R}^p\) of the hazard function, we can add a penalty
to the likelihood and optimise the following equation: \begin{equation}
\min _{\theta \in \mathbb{R}^{p}}\,\,-\ell\left(\theta\right)+\sum_{j=1}^p w_j P(\theta_j;\lambda,\alpha) \label{eq:penest}
\end{equation} where \(\ell\left(\theta\right) = \log L(\theta)\) is the
log of the likelihood function given in \eqref{eq:lik-function},
\(P(\theta_j;\lambda,\alpha)\) is a penalty term controlled by the
non-negative regularization parameters \(\lambda\) and \(\alpha\), and
\(w_j\) is the penalty factor for the \(j\)th covariate. These penalty
factors serve as a way of allowing parameters to be penalized
differently. For example, we could set the penalty factor for time to be
0 to ensure it is always included in the selected model.

\hypertarget{comparison-with-existing-packages}{%
\section{Comparison with existing
packages}\label{comparison-with-existing-packages}}

Survival analysis is an important branch of applied statistics and
epidemiology. Accordingly, there is already a vast ecosystem of R
packages implementing different methodologies. In this section, we
describe how the functionalities of \pkg{casebase} compare to these
packages.

At the time of writing, a cursory examination of CRAN's \ctv{Survival}
Task View reveals that there are over 250 packages related to survival
analysis \citep{survTaskView}. For the purposes of this article, we
restricted our review to packages that implement at least one of the
following features: parametric modeling, non-proportional hazard models,
competing risk analysis, penalized estimation, and Cumulative Incidence
(CI) estimation. By searching for appropriate keywords in the
\code{DESCRIPTION} file of these packages, we found 60 relevant
packages. These 60 packages were then manually examined to determine
which ones are comparable to \pkg{casebase}. In particular, we excluded
packages that were focused on a different set of problems, such as
frailty and multistate models. The remaining 14 packages appear in Table
\ref{tab:surv-pkgs}, along with some of the functionalities they offer.

Parametric survival models are implemented in several packages, each
differing in the parametric distributions available: \CRANpkg{CFC}
\citeyearpar{mahani2015bayesian}, \CRANpkg{flexsurv}
\citeyearpar{flexsurv}, \CRANpkg{SmoothHazard}
\citeyearpar{smoothHazard}, \CRANpkg{rstpm2} \citeyearpar{clements_liu},
\CRANpkg{mets} \citeyearpar{scheike2014estimating}, and
\CRANpkg{survival} \citeyearpar{survival-package}. For example,
\pkg{SmoothHazard} is limited to Weibull distributions
\citeyearpar{smoothHazard}, whereas both \pkg{flexsurv} and
\pkg{survival} allow users to supply any distribution of their choice.
\pkg{flexsurv}, \pkg{SmoothHazard}, \pkg{mets} and \pkg{rstpm2} can
model the effect of time using splines, which allows flexible modeling
of the hazard function. As discussed above, \pkg{casebase} can model any
parametric family whose log-hazard can be expressed as a linear
combination of covariates (including time). Therefore, our package is
more general in that it allows the user to model any linear or
non-linear transformation of time including splines and higher order
polynomials. Also, by including interaction terms between covariates and
time, it also allows users to fit (non-proportional) time-varying
coefficient models. However, unlike \pkg{flexsurv}, we do not explicitly
model any shape parameter.

Several packages implement penalized estimation for the Cox model:
\CRANpkg{glmnet} \citeyearpar{regpathcox}, \CRANpkg{glmpath}
\citeyearpar{park_hastie}, \CRANpkg{penalized} \citeyearpar{l1penal},
\CRANpkg{riskRegression} \citeyearpar{gerds_blanche}. Moreover, some
packages also include penalized estimation in the context of Cox models
with time-varying coefficients: elastic-net penalization with
\pkg{rstpm2} \citeyearpar{clements_liu}, while \pkg{survival}
\citeyearpar{survival-package} has an implementation of ridge-penalized
estimation. On the other hand, our package \pkg{casebase} provides
penalized estimation of the hazard function. To our knowledge,
\pkg{casebase} and \pkg{rstpm2} are the only packages to offer this
functionality.

Next, several R packages implement methodologies for competing risk
analysis; for a different perspective on this topic, see Mahani \&
Sharabiani \citeyearpar{mahani2015bayesian}. The package \pkg{survival}
provides functionality for competing-risk analysis and multistate
modelling. The package \CRANpkg{cmprsk} provides methods for
cause-specific subdistribution hazards, such as in the Fine-Gray model
\citeyearpar{fine1999proportional}. On the other hand, the package
\pkg{CFC} estimates cause-specific CIs from unadjusted, non-parametric
survival functions. Our package \pkg{casebase} also provides
functionalities for competing risk analysis by estimating parametrically
the cause-specific hazards. From these quantities, we can then estimate
the cause-specific CIs.

Finally, several packages include functions to estimate the survival
function and the CI. The corresponding methods generally fall into two
categories: transformation of the estimated hazard function, and
semi-parametric estimation of the baseline hazard. The first category
broadly corresponds to parametric survival models, where the full hazard
is explicitly modeled. Using this estimate, the survival function and
the CI can be obtained using their functional relationships (see
Equations \ref{eqn:surv} and \ref{eqn:CDF} below). Packages providing
this functionality include \pkg{CFC}, \pkg{flexsurv}, \pkg{mets}, and
\pkg{survival}. Our package \pkg{casebase} also follows this approach
for both single-event and competing-risk analyses. The second category
outlined above broadly corresponds to semi-parametric models. These
models do not model the full hazard function, and therefore the baseline
hazard needs to be estimated separately in order to estimate the
survival function. This is achieved using semi-parametric estimators
(e.g.~Breslow's estimator) or parametric estimators (e.g.~spline
functions). Packages that implement this approach include
\pkg{riskRegression}, \pkg{rstpm2}, \pkg{survival}, and \pkg{glmnet}. As
mentioned in the introduction, a key distinguishing factor between these
two approaches is that the first category leads to smooth estimates of
the survival function, whereas the second category often produces
estimates in the form of stepwise functions.

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccccc}
\toprule
& \textbf{Competing} & \textbf{Allows} &  \textbf{Penalized}   &    &  & \textbf{Semi} & \textbf{Interval/Left} & \textbf{Risk}  \\
\textbf{Package}        & \textbf{Risks}  & \textbf{Non PH} & \textbf{Regression} & \textbf{Splines} & \textbf{Parametric} & \textbf{Parametric} & \textbf{Censoring} & \textbf{Estimates}           \\
\bottomrule
\textbf{casebase}        & \checkmark                        & \checkmark                         & \checkmark                     & \checkmark                & \checkmark                   &                          &                                  & \checkmark                                \\ \hline
\textbf{CFC}             & \checkmark                        & \checkmark                         &                       &                  & \checkmark                   &                          &                                  & \checkmark                                \\ \hline
\textbf{cmprsk}             & \checkmark                        &                           &                       &                  &                     &  \checkmark                        &                                  & \checkmark                                \\ \hline
\textbf{crrp}            & \checkmark                        &                           & \checkmark                     &                  &                     & \checkmark                        &                                  &                                  \\ \hline
\textbf{fastcox}         &                          &                           & \checkmark                     &                  &                     & \checkmark                        &                                  &                                  \\ \hline
\textbf{flexrsurv}       &                          & \checkmark                         &                       & \checkmark                & \checkmark                   &                          &                                  & \checkmark               \\ \hline
\textbf{flexsurv}        & \checkmark                        & \checkmark                         &                       & \checkmark                & \checkmark                   &                          &                                  & \checkmark               \\ \hline
\textbf{glmnet}          &                          &                           & \checkmark                     &                  &                     & \checkmark                        &                                  &  \checkmark              \\ \hline
\textbf{glmpath}         &                          &                           & \checkmark                     &                  &                     & \checkmark                        &                                  &                                  \\ \hline
\textbf{mets}            & \checkmark                        &                           &                       & \checkmark                &                     & \checkmark                        &                                  & \checkmark                                \\ \hline
\textbf{penalized}       &                          &                           & \checkmark                     &                  &                     & \checkmark                        &                                  &                                  \\ \hline
\textbf{riskRegression}  & \checkmark                         &                           & \checkmark                     &                  &                     & \checkmark                        &                                  & \checkmark                                \\ \hline
\textbf{rstpm2}          &                          & \checkmark                         &                      & \checkmark                & \checkmark                   & \checkmark                        & \checkmark                                & \checkmark                         \\ \hline
\textbf{SmoothHazard}    &                          & \checkmark                         &                       & \checkmark                & \checkmark                   &                          & \checkmark                            &                                      \\ \hline
\textbf{survival}        & \checkmark                        & \checkmark                         &                       &                  & \checkmark                   & \checkmark                        & \checkmark                                & \checkmark                               \\ 
\bottomrule
\end{tabular}%
}
\caption{Comparison of various R packages for survival analysis. \textbf{Competing Risks}: whether an implementation for competing risks is present. \textbf{Allows Non PH}: includes models for non-proportional hazards. \textbf{Penalized Regression}: allows for a penalty term on the regression coefficients when estimating hazards (e.g. lasso or ridge). \textbf{Splines}: allows a flexible fit on time through the use of splines. \textbf{Parametric}: implementation for parametric models. \textbf{Semi-parametric}: implementation for semi-parametric models. \textbf{Interval/left censoring}: models for interval and left-censoring. If this is not selected, the package only handles right-censoring. \textbf{Risk estimates}: estimation of survival curve and cumulative incidence is available.}
\label{tab:surv-pkgs}
\end{table}

\hypertarget{implementation-details}{%
\section{Implementation details}\label{implementation-details}}

The functions in the \pkg{casebase} package can be divided into two
categories: 1) exploratory data analysis, in the form of population-time
plots; and 2) parametric modeling of the hazard function. We strove for
compatibility with both \code{data.frame}s and \code{data.table}s; this
can be seen in the coding choices we made and the unit tests we wrote.

\hypertarget{population-time-plots}{%
\subsection{Population-time plots}\label{population-time-plots}}

Population-time plots are a descriptive visualization of incidence
density, where the study base is represented by area and events by
points within the area. The case-base sampling approach described above
can be visualized in the form of a population time plot. These plots are
informative graphical displays of survival data and should be one of the
first steps in an exploratory data analysis. The \code{popTime} function
and \code{plot} method facilitate this task:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The \code{casebase::popTime} function takes as input the original
  dataset along with the column names corresponding to the timescale,
  the event status and an exposure group of interest (optional). This
  will create an object of class \code{popTime}.\\
\item
  The corresponding \code{plot} method for the object created in Step 1
  can be called to create the population time plot with several options
  for customizing the aesthetics.
\end{enumerate}

By splitting these tasks, we give flexibility to the user. While the
method call in Step 2 allows further customization by using the
\CRANpkg{ggplot2} \citep{ggplot2} family of functions, users may choose
the graphics system of their choice to create population-time plots from
the object created in Step 1.

To illustrate these functions, we will use data from the European
Randomized Study of Prostate Cancer Screening (ERSPC)
\citep{schroder2009screening} which was extracted using the approach
described in Liu \emph{et al.} \citeyearpar{liu2014recovering}. This
dataset is available through the \pkg{casebase} package. It contains the
individual observations for 159,893 men from seven European countries,
who were between the ages of 55 and 69 years when recruited for the
trial.

We first create the necessary dataset for producing the population time
plot using the \code{popTime} function. In this example, we stratify the
plot by treatment group. The resulting object inherits from class
\code{popTime} and stores the exposure variable as an attribute:

\begin{Schunk}
\begin{Sinput}
pt_object <- casebase::popTime(ERSPC, time = "Follow.Up.Time",
                               event = "DeadOfPrCa", exposure = "ScrArm")
inherits(pt_object, "popTime")
\end{Sinput}
\begin{Soutput}
#> [1] TRUE
\end{Soutput}
\begin{Sinput}
attr(pt_object, "exposure")
\end{Sinput}
\begin{Soutput}
#> [1] "ScrArm"
\end{Soutput}
\end{Schunk}

\begin{Schunk}
\begin{figure}[ht]
\includegraphics[width=\textwidth,keepaspectratio=true]{./plot-erspc-data-1} \caption{Population time plot for the ERSPC dataset. \textbf{A}: The gray area can be thought of as N = 159,893 infinitely thin horizontal rectangles ordered by length of follow-up. \textbf{B}: The red points correspond to when death has occurred for any one of those infinitely thin rectangles. \textbf{C}: To improve visibility, these red points are randomly redistributed along their respective x-coordinates, providing a visualization of incidence density. More events are observed at later follow-up times, motivating the use of non-constant hazard models. \textbf{D}: The base series, a representative sample of the entire grey area, is represented by the green points.}\label{fig:plot-erspc-data}
\end{figure}
\end{Schunk}

We then pass this object to the corresponding \code{plot} method:

\begin{Schunk}
\begin{Sinput}
plot(pt_object, add.base.series = TRUE)
\end{Sinput}
\end{Schunk}

Figure \ref{fig:plot-erspc-data} depicts the process of creating a
population-time plot. It is built sequentially by first adding a layer
for the area representing the population time in gray (Figure
\ref{fig:plot-erspc-data} A), with subjects having the least amount of
observation time plotted at the top of the y-axis. We immediately notice
a distinctive \emph{stepwise shape} in the population time area. This is
due to the randomization of the Finnish cohorts which were carried out
on January 1 of each of year from 1996 to 1999. Coupled with the uniform
December 31 2006 censoring date, this led to large numbers of men with
exactly 11, 10, 9 or 8 years of follow-up. Tracked backwards in time
(i.e.~from right to left), the population-time plot shows the
recruitment pattern from its beginning in 1991, and the January 1
entries in successive years. Tracked forwards in time (i.e.~from left to
right), the plot for the first three years shows attrition due entirely
to death (mainly from other causes). Since the Swedish and Belgian
centres were the last to complete recruitment in December 2003, the
minimum potential follow-up is three years. Tracked further forwards in
time (i.e.~after year 3) the attrition is a combination of deaths and
staggered entries. As we can see, population-time plots summarise a
wealth of information about the study into a simple graph.

Next, layers for the case series and base series are added. The y-axis
location of each case moment is sampled at random vertically on the plot
to avoid having all points along the upper edge of the gray area (Figure
\ref{fig:plot-erspc-data} B). By randomly distributing the cases, we can
get a sense of the incidence density. In Figure
\ref{fig:plot-erspc-data} C, we see that more events are observed at
later follow-up times. Therefore, a constant hazard model would not be
appropriate in this instance as it would overestimate the incidence
earlier on in time, and underestimate it later on. Finally, the base
series is sampled uniformly from the study base (Figure
\ref{fig:plot-erspc-data} D). The reader should refer to the package
vignettes for more examples and a detailed description of how to modify
the aesthetics of a population-time plot.

\hypertarget{parametric-modeling}{%
\subsection{Parametric modeling}\label{parametric-modeling}}

The parametric modeling step was separated into three parts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  case-base sampling;
\item
  estimation of the smooth hazard function;
\item
  estimation of the survival function.
\end{enumerate}

By separating the sampling and estimation functions, we allow the
possibility of users implementing more complex sampling scheme (as
described in Saarela \citeyearpar{saarela2016case}), or more complex
study designs (e.g.~time-varying exposure).

The sampling scheme selected for \code{sampleCaseBase} was described in
Hanley \& Miettinen \citeyearpar{hanley2009fitting}: we first sample
along the ``person'' axis, proportional to each individual's total
follow-up time, and then we sample a moment uniformly over their
follow-up time. This sampling scheme is equivalent to the following
picture: imagine representing the total follow-up time of all
individuals in the study along a single dimension, where the follow-up
time of the next individual would start exactly when the follow-up time
of the previous individual ends. Then the base series could be sampled
uniformly from this one-dimensional representation of the overall
follow-up time. In any case, the output is a dataset of the same class
as the input, where each row corresponds to a person-moment. The
covariate profile for each such person-moment is retained, and an offset
term is added to the dataset. This output could then be used to fit a
smooth hazard function, or for visualization of the base series.

Next, the fitting function \code{fitSmoothHazard} starts by looking at
the class of the dataset: if it was generated from
\code{sampleCaseBase}, it automatically inherited the class
\code{cbData}. If the dataset supplied to \code{fitSmoothHazard} does
not inherit from \code{cbData}, then the fitting function starts by
calling \code{sampleCaseBase} to generate the base series. In other
words, users can bypass \code{sampleCaseBase} altogether and only worry
about the fitting function \code{fitSmoothHazard}.

The fitting function retains the familiar formula interface of
\code{glm}. The left-hand side of the formula should be the name of the
column corresponding to the event type. The right-hand side can be any
combination of the covariates, along with an explicit functional form
for the time variable. Note that non-proportional hazard models can be
achieved at this stage by adding an interaction term involving time
(cf.~Case Study 1 below). The offset term does not need to be specified
by the user, as it is automatically added to the formula before calling
\code{glm}.

To fit the hazard function, we provide several approaches that are
available via the \code{family} parameter. These approaches are:

\begin{itemize}
\tightlist
\item
  \code{glm}: This is the familiar logistic regression.
\item
  \code{glmnet}: This option allows for variable selection using the
  elastic-net \citep{zou2005regularization} penalty (cf.~Case Study 3).
  This functionality is provided through the \pkg{glmnet} package
  \citep{friedman2010jss}.
\item
  \code{gam}: This option provides support for \emph{Generalized
  Additive Models} via the \CRANpkg{mgcv} package
  \citep{hastie1987generalized}.
\end{itemize}

In the case of multiple competing events, the hazard is fitted via
multinomial regression as performed by the \CRANpkg{VGAM} package. We
selected this package for its ability to fit multinomial regression
models with an offset.

Once a model-fit object has been returned by \code{fitSmoothHazard}, all
the familiar summary and diagnostic functions are available:
\code{print}, \code{summary}, \code{predict}, \code{plot}, etc. Our
package provides one more functionality: it computes risk functions from
the model fit. For the case of a single event, it uses the familiar
identity \begin{equation}\label{eqn:surv}
S(t) = \exp\left(-\int_0^t \lambda(u;X) du\right).
\end{equation} The integral is computed using either the numerical or
Monte-Carlo integration. The risk function (or cumulative distribution
function) is then defined as \begin{equation}\label{eqn:CDF}
F(t) = 1 - S(t).
\end{equation}

For the case of a competing-event analysis, the event-specific risk is
computed using the following procedure: first, we compute the overall
survival function (i.e.~for all event types):

\[ S(t) = \exp\left(-\int_0^t \lambda(u;X) du\right),\qquad \lambda(t;X) = \sum_{j=1}^J \lambda_j(t;X).\]
From this, we can derive the event-specific subdensities:

\[ f_j(t) = \lambda_j(t)S(t).\]

By integrating these subdensities, we obtain the event-specific CI
functions:

\[ CI_j(t) = \int_0^t f_j(u)du.\] Again, the integrals are computed
using either numerical integration (via the trapezoidal rule) or Monte
Carlo integration. This option is controlled by the argument
\code{method} of the \code{absoluteRisk} function.

Finally, the output from \code{absoluteRisk} can be passed to a method
\code{confint} to compute confidence bands around the survival function.
These bands are computed using parametric bootstrap, and therefore are
only valid when \code{family = "glm"} as it relies on the asymptotic
normality of the estimator. Currently, this is only available for the
single-event setting.

\hypertarget{illustration-of-package}{%
\section{Illustration of package}\label{illustration-of-package}}

In this section, we illustrate the main functions of the \pkg{casebase}
package through three case studies. Each one showcases a different type
of analysis. First, we show how to model time flexibly as well as
non-proportional hazards. Then we perform a competing-risk analysis and
compare our results with the Cox model and the Fine-Gray model. The
third case study illustrates how to perform variable selection in
high-dimensional datasets.

\hypertarget{case-study-1modeling-different-functions-of-time-and-non-proportional-hazards}{%
\subsection{Case study 1---Modeling different functions of time and
non-proportional
hazards}\label{case-study-1modeling-different-functions-of-time-and-non-proportional-hazards}}

For our first case study, we return to the ERSPC study and investigate
the differences in risk between the control and screening arms. Previous
re-analyses of these data suggest that the 20\% reduction in prostate
cancer death due to screening was an underestimate
\citep{hanley2010mortality}. The estimated 20\% (from a proportional
hazards model) did not account for the delay between screening and the
time the effect is expected to be observed. As a result, the null
effects in years 1--7 masked the substantial reductions that began to
appear from year 8 onward. This motivates the use of a time-dependent
hazard ratio which can easily be fit with the \pkg{casebase} package by
including an interaction term with time in the model. We fit a flexible
hazard by using a smooth function of time modeled with a penalised cubic
spline basis with 2 degrees of freedom. The model is fit using
\code{fitSmoothHazard} with the familiar formula interface:

\begin{Schunk}
\begin{Sinput}
library(survival) # for the pspline function
fit <- fitSmoothHazard(DeadOfPrCa ~ pspline(Follow.Up.Time, df = 2) * ScrArm, 
                       data = ERSPC, ratio = 100)
\end{Sinput}
\end{Schunk}

The output object from \code{fitSmoothHazard} inherits from the
\code{singleEventCB} and \code{glm} classes. For this reason, we can
leverage the \code{summary} method for \code{glm} objects to output a
familiar summary of the results:

\begin{Schunk}
\begin{Sinput}
summary(fit) 
\end{Sinput}
\begin{Soutput}
#> Fitting smooth hazards with case-base sampling
#> 
#> Sample size: 159893 
#> Number of events: 540 
#> Number of base moments: 54000 
#> ----
#> 
#> Call:
#> fitSmoothHazard(formula = DeadOfPrCa ~ pspline(Follow.Up.Time, 
#>     df = 2) * ScrArm, data = ERSPC, ratio = 100)
#> 
#> Deviance Residuals: 
#>    Min      1Q  Median      3Q     Max  
#> -0.477  -0.164  -0.140  -0.084   3.882  
#> 
#> Coefficients:
#>                                                        Estimate Std. Error
#> (Intercept)                                             -12.613      9.756
#> pspline(Follow.Up.Time, df = 2)1                          1.320     10.705
#> pspline(Follow.Up.Time, df = 2)2                          5.190      9.526
#> pspline(Follow.Up.Time, df = 2)3                          4.481      9.866
#> pspline(Follow.Up.Time, df = 2)4                          6.060      9.690
#> pspline(Follow.Up.Time, df = 2)5                          5.231      9.851
#> pspline(Follow.Up.Time, df = 2)6                          9.176      9.749
#> pspline(Follow.Up.Time, df = 2)7                         -0.434     19.572
#> ScrArmScreening group                                     7.900     13.050
#> pspline(Follow.Up.Time, df = 2)1:ScrArmScreening group   -7.962     14.507
#> pspline(Follow.Up.Time, df = 2)2:ScrArmScreening group   -8.351     12.690
#> pspline(Follow.Up.Time, df = 2)3:ScrArmScreening group   -7.787     13.224
#> pspline(Follow.Up.Time, df = 2)4:ScrArmScreening group   -8.098     12.942
#> pspline(Follow.Up.Time, df = 2)5:ScrArmScreening group   -9.389     13.223
#> pspline(Follow.Up.Time, df = 2)6:ScrArmScreening group   -7.385     13.114
#> pspline(Follow.Up.Time, df = 2)7:ScrArmScreening group  -17.380     30.806
#>                                                        z value Pr(>|z|)
#> (Intercept)                                              -1.29     0.20
#> pspline(Follow.Up.Time, df = 2)1                          0.12     0.90
#> pspline(Follow.Up.Time, df = 2)2                          0.54     0.59
#> pspline(Follow.Up.Time, df = 2)3                          0.45     0.65
#> pspline(Follow.Up.Time, df = 2)4                          0.63     0.53
#> pspline(Follow.Up.Time, df = 2)5                          0.53     0.60
#> pspline(Follow.Up.Time, df = 2)6                          0.94     0.35
#> pspline(Follow.Up.Time, df = 2)7                         -0.02     0.98
#> ScrArmScreening group                                     0.61     0.54
#> pspline(Follow.Up.Time, df = 2)1:ScrArmScreening group   -0.55     0.58
#> pspline(Follow.Up.Time, df = 2)2:ScrArmScreening group   -0.66     0.51
#> pspline(Follow.Up.Time, df = 2)3:ScrArmScreening group   -0.59     0.56
#> pspline(Follow.Up.Time, df = 2)4:ScrArmScreening group   -0.63     0.53
#> pspline(Follow.Up.Time, df = 2)5:ScrArmScreening group   -0.71     0.48
#> pspline(Follow.Up.Time, df = 2)6:ScrArmScreening group   -0.56     0.57
#> pspline(Follow.Up.Time, df = 2)7:ScrArmScreening group   -0.56     0.57
#> 
#> (Dispersion parameter for binomial family taken to be 1)
#> 
#>     Null deviance: 6059.0  on 54539  degrees of freedom
#> Residual deviance: 5772.7  on 54524  degrees of freedom
#> AIC: 5805
#> 
#> Number of Fisher Scoring iterations: 9
\end{Soutput}
\end{Schunk}

As noted in the Theoretical Details section, the usual asymptotic
results hold for likelihood ratio tests built using case-base sampling
models. Therefore, we can easily test the significance of the spline
model:

\begin{Schunk}
\begin{Soutput}
#> Analysis of Deviance Table
#> 
#> Model: binomial, link: logit
#> 
#> Response: DeadOfPrCa
#> 
#> Terms added sequentially (first to last)
#> 
#> 
#>                                        Df Deviance Resid. Df Resid. Dev
#> NULL                                                   54539       6059
#> pspline(Follow.Up.Time, df = 2)         7    269.1     54532       5790
#> ScrArm                                  1      8.0     54531       5782
#> pspline(Follow.Up.Time, df = 2):ScrArm  7      9.1     54524       5773
#>                                        Pr(>Chi)    
#> NULL                                               
#> pspline(Follow.Up.Time, df = 2)          <2e-16 ***
#> ScrArm                                   0.0046 ** 
#> pspline(Follow.Up.Time, df = 2):ScrArm   0.2454    
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{Soutput}
\end{Schunk}

The null hypothesis is rejected in favor of the spline model. Similarly,
to compare different models (e.g.~time modeled linearly), we could
compute Akaike's Information Criterion (AIC) for each model and compare
them.

\hypertarget{time-dependent-hazard-ratios}{%
\subsubsection{Time-dependent hazard
ratios}\label{time-dependent-hazard-ratios}}

Note that we did not have to specify any cut points, as would be the
case with the \code{survSplit} function in the \pkg{survival} package.
In Figure \ref{fig:interaction-ERSPC}, we have the estimated hazard
ratio and 95\% confidence interval for screening vs.~control group as a
function of time using the \code{plot} method for objects of class
\code{singleEventCB}:

\begin{Schunk}
\begin{Sinput}
new_time <- seq(1, 12, by  = 0.1)
new_data <- data.frame(ScrArm = factor("Control group",
                                         levels = c("Control group","Screening group")),
                      Follow.Up.Time = new_time)
plot(fit, type = "hr", newdata = new_data,
     var = "ScrArm", xvar = "Follow.Up.Time", ci = F)
\end{Sinput}
\end{Schunk}

The plot shows that the effect of screening only becomes statistically
apparent by year 7 and later. The 25-60\% reductions seen in years 8-12
of the study suggests a much higher reduction in prostate cancer due to
screening than the single overall 20\% reported in the original article.

\begin{Schunk}
\begin{figure}[ht]
\includegraphics[width=\textwidth,keepaspectratio=true]{./interaction-ERSPC-1} \caption[Estimated hazard ratio and 95\% confidence interval for screening vs]{Estimated hazard ratio and 95\% confidence interval for screening vs. control group as a function of time in the ERSPC dataset. Hazard ratios are estimated from fitting a parametric hazard model as a function of the interaction between a cubic B-spline basis of follow-up time and treatment arm. 95\% confidence intervals are calculated using the delta method. The plot shows that the effect of screening only begins to become statistically apparent by year 7. The 25-60\% reductions seen in years 8-12 of the study suggests a much higher reduction in prostate cancer due to screening than the single overall 20\% reported in the original article.}\label{fig:interaction-ERSPC}
\end{figure}
\end{Schunk}

\bibliography{bhatnagar-turgeon-islam-hanley-saarela.bib}

\address{%
Sahir Rai Bhatnagar*\\
McGill University\\%
1020 Pine Avenue West Montreal, QC, Canada H3A 1A2\\
%
\url{http://sahirbhatnagar.com/}%
%
\\\email{sahir.bhatnagar@mcgill.ca}
}

\address{%
Maxime Turgeon*\\
University of Manitoba\\%
186 Dysart Road Winnipeg, MB, Canada R3T 2N2\\
%
\url{https://maxturgeon.ca/}%
%
\\\email{max.turgeon@umanitoba.ca}
}

\address{%
Jesse Islam\\
McGill University\\%
1020 Pine Avenue West Montreal, QC, Canada H3A 1A2\\
%
%
%
\\\email{jesse.islam@mail.mcgill.ca}
}

\address{%
James A. Hanley\\
McGill University\\%
1020 Pine Avenue West Montreal, QC, Canada H3A 1A2\\
%
\url{http://www.medicine.mcgill.ca/epidemiology/hanley/}%
%
\\\email{james.hanley@mcgill.ca}
}

\address{%
Olli Saarela\\
University of Toronto\\%
Dalla Lana School of Public Health, 155 College Street, 6th floor,
Toronto, Ontario M5T 3M7, Canada\\
%
\url{http://individual.utoronto.ca/osaarela/}%
%
\\\email{olli.saarela@utoronto.ca}
}
